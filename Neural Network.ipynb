{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick demonstration of the neural network implementation given in this repository. We use the well known MNIST handwritten numeral dataset, which consists of 28 pixel by 28 pixel images of handwritten numerals from 0 to 9. The data thus has 784 features, each being an intensity value from 0 (white) to 255 (black). There are 70000 images in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame, Series\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_cg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is included here for reference. We will only be calling the train_network function, and the predict method from the NeuralNetModel class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel():\n",
    "    \"\"\" Output of a neural network training algorithm.\"\"\"\n",
    "    def __init__(self, neural_network):\n",
    "        self._network = neural_network\n",
    "        \n",
    "    def get_weights(self):\n",
    "        \"\"\" Return the weights used by the model to make predictions.\"\"\"\n",
    "        return self._network.weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify the observations in X.\n",
    "        \n",
    "        Args: \n",
    "            X (array): data to be classified. \n",
    "        Returns:\n",
    "            array: each entry is the class assigned to the corresponding\n",
    "            row in X.\n",
    "        Raises:\n",
    "            ValueError: if the features in X do not match the model. \n",
    "        \"\"\"\n",
    "        \n",
    "        num_features = self._network.layers[0]\n",
    "        if not X.shape[1] == num_features:\n",
    "            raise ValueError(\"Number of features in data must be {0}.\"\n",
    "                             .format(num_features))\n",
    "        forward_propogate(self.get_weights(), X, self._network)\n",
    "        class_predictions = self._network.get_final_activation()\n",
    "        return np.argmax(class_predictions, axis = 1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(\"Trained neural network with {0} hidden layers of size(s) {1}.\"\n",
    "               .format(len(self._network.layers)-2, self._network.layers[1:-1]))\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\" Internal network class to keep track of its properties.\"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = layer_sizes\n",
    "        self._shapes = []\n",
    "        self.activations = []\n",
    "        self.raw_outputs = []\n",
    "        self._initialize_shapes()\n",
    "        self.weights = None\n",
    "        \n",
    "    def num_transitions(self):\n",
    "        \"\"\" Return the number of transitions between layers.\"\"\"\n",
    "        return len(self.layers) - 1\n",
    "    \n",
    "    def get_final_activation(self):\n",
    "        \"\"\" Return the last activation, giving the current probabilities.\"\"\"\n",
    "        return self.activations[self.num_transitions()]\n",
    "        \n",
    "    def _initialize_shapes(self):\n",
    "        \"\"\" Determine the shapes for the weights in the network.\"\"\"\n",
    "        for index in range(self.num_transitions()):\n",
    "            input_size = self.layers[index]\n",
    "            output_size = self.layers[index+1]\n",
    "            shape = (output_size, 1 + input_size)\n",
    "            self._shapes.append(shape)\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\" Return random initial weights according to this network's shapes.\"\"\"\n",
    "        weights = []\n",
    "        for index in range(self.num_transitions()):\n",
    "            init_epsilon = self._get_initial_epsilon(index)\n",
    "            shape_x, shape_y = self._shapes[index]\n",
    "            weight = init_epsilon * (np.random.rand(shape_x, shape_y) * 2 - 1)\n",
    "            weights.append(weight)\n",
    "        return weights\n",
    "        \n",
    "    def _get_initial_epsilon(self, index):\n",
    "        \"\"\" Return scaling factor for initialized weights.\"\"\"\n",
    "        input_size = self.layers[index]\n",
    "        output_size = self.layers[index+1]\n",
    "        return (6 / (input_size + output_size))**(1/2)\n",
    "        \n",
    "    def reshape_weights(self, flat_weights):\n",
    "        \"\"\" Restore shape of weights according to this network's shapes.\"\"\"\n",
    "        start_index = 0\n",
    "        shaped_weights = []\n",
    "        for shape in self._shapes:\n",
    "            matrix_size = shape[0] * shape[1]\n",
    "            end_index = start_index + matrix_size\n",
    "            shaped_weights.append(flat_weights[start_index:end_index].reshape(shape))\n",
    "            start_index = end_index\n",
    "        return shaped_weights\n",
    "        \n",
    "def train_network(X, Y, layers, regularization = 0, max_iters = 200):\n",
    "    \"\"\" Train a neural network and return the model.\n",
    "    \n",
    "    Args:\n",
    "        X (array): data consisting of rows of features. \n",
    "        Y (array): array of labels corresponding to each row in X.\n",
    "            Must consist of integers from 0 to n for some integer n.\n",
    "        layers (list): the number of features in each layer. The first\n",
    "            entry must be the number of features (columns) in X, the \n",
    "            last must be the number of classes, and those inbetween\n",
    "            determine the size of each hidden layer.\n",
    "        regularization (int): penalty factor for having larger weights.\n",
    "            (defualt: 0).\n",
    "        max_iters (int): the max number of iterations used by the algorithm\n",
    "            when searching for optimal weights. A higher number will produce\n",
    "            a better fit but extends run time (default: 200).\n",
    "    \"\"\"\n",
    "    check_input_validity(X, Y, layers)\n",
    "    num_classes = layers[-1]\n",
    "    network = NeuralNetwork(layers)\n",
    "    initial_weights = flatten_weights(network.initialize_weights()) \n",
    "    Y = process_labels(Y, num_classes)\n",
    "    optimal = fmin_cg(compute_cost, \n",
    "                      initial_weights, \n",
    "                      back_propogate, \n",
    "                      args = (X, Y, network, regularization), \n",
    "                      maxiter = max_iters)\n",
    "    forward_propogate(network.reshape_weights(optimal), X, network)\n",
    "    network.weights = network.reshape_weights(optimal)\n",
    "    return NeuralNetModel(network)\n",
    "    \n",
    "def compute_cost(flat_weights, X, label_matrix, network, regularization):\n",
    "    \"\"\" Propogate weights through network and compute cost function.\"\"\"\n",
    "    weights = network.reshape_weights(flat_weights)\n",
    "    forward_propogate(weights, X, network)\n",
    "    return cost_function(weights, label_matrix, network, regularization)\n",
    "    \n",
    "def forward_propogate(weights, X, network):\n",
    "    \"\"\" Perform forward propogation on the given network and dataset.\"\"\"\n",
    "    raw_outputs = [X]\n",
    "    activations = [X]\n",
    "    for i in range(network.num_transitions()):\n",
    "        activations[i] = insert_ones(activations[i])\n",
    "        weight = weights[i]\n",
    "        raw_output = activations[i].dot(weight.transpose())\n",
    "        activation = sigmoid(raw_output)\n",
    "        raw_outputs.append(raw_output)\n",
    "        activations.append(activation)\n",
    "    network.activations = activations\n",
    "    network.raw_outputs = raw_outputs\n",
    "        \n",
    "def cost_function(weights, label_matrix, network, regularization):\n",
    "    \"\"\" Compute the cost function for the network's current state.\"\"\"\n",
    "    a = network.get_final_activation()\n",
    "    Y = label_matrix\n",
    "    m = len(label_matrix)\n",
    "    weight_sum = 0\n",
    "    for weight in weights:\n",
    "        weight_sum += (weight[:,1:]**2).sum()\n",
    "    reg_term = (regularization / (2*m)) * weight_sum\n",
    "    return (-Y * log(a) - (1-Y) * log(1-a)).sum() / m + reg_term\n",
    "    \n",
    "def back_propogate(flat_weights, X, label_matrix, network, regularization):\n",
    "    \"\"\" Use back propogation to get the gradient of the cost function.\"\"\"\n",
    "    weights = network.reshape_weights(flat_weights)\n",
    "    if len(network.activations) == 0:\n",
    "        forward_propogate(weights, X, network)\n",
    "    deltas = get_deltas(weights, label_matrix, network)\n",
    "    weight_gradients = get_weight_gradients(weights, deltas, network, regularization)\n",
    "    return weight_gradients\n",
    "\n",
    "def get_deltas(weights, label_matrix, network):\n",
    "    \"\"\" Return a list of the deltas needed for the gradient computation.\"\"\"\n",
    "    deltas = []\n",
    "    delta = network.get_final_activation() - label_matrix\n",
    "    deltas.append(delta)\n",
    "    for index in reversed(range(1, network.num_transitions())):\n",
    "        weight = weights[index][:,1:]\n",
    "        sigmoid_grad = sigmoid_gradient(network.raw_outputs[index])\n",
    "        delta = delta.dot(weight) * sigmoid_grad\n",
    "        deltas.insert(0, delta)\n",
    "    return deltas\n",
    "\n",
    "def get_weight_gradients(weights, deltas, network, regularization):\n",
    "    \"\"\" Return a flat array of the gradients of the weights.\"\"\"\n",
    "    activations = network.activations\n",
    "    weight_gradients = []\n",
    "    m = activations[0].shape[0]\n",
    "    for index, weight in enumerate(weights):\n",
    "        weight[:,0] = 0\n",
    "        delta, activation = deltas[index], activations[index]\n",
    "        base_term = delta.transpose().dot(activation) / m\n",
    "        reg_term = regularization * weight / m\n",
    "        weight_gradients.append(base_term + reg_term)\n",
    "    return flatten_weights(np.array(weight_gradients))\n",
    "\n",
    "def flatten_weights(weights):\n",
    "    \"\"\" Return a flat array of the weights.\"\"\"\n",
    "    return np.concatenate([weight.flatten() for weight in weights])\n",
    "\n",
    "def process_labels(Y, num_labels):\n",
    "    \"\"\" Given a sequence of labels 0 to n, produce a 0-1 matrix where entry\n",
    "    i, j is 1 if and only if the ith label is j.\"\"\"\n",
    "    label_matrix = np.zeros((len(Y), num_labels))\n",
    "    for i in range(num_labels):\n",
    "        label_matrix[:,i] = 1 * (Y == i)\n",
    "    return label_matrix\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    \"\"\" Gradient of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\" Vectorized sigmoid/logistic function.\"\"\"\n",
    "    return expit(z)\n",
    "    \n",
    "def insert_ones(X):\n",
    "    \"\"\" Insert a column of ones in front of the dataset X and return it.\"\"\"\n",
    "    X = array_to_ndarray(X)\n",
    "    num_rows = X.shape[0]\n",
    "    return np.hstack((np.ones((num_rows, 1)), X))\n",
    "    \n",
    "def array_to_ndarray(X):\n",
    "    \"\"\" Return a multidimensional version of X if it isn't already one.\"\"\"\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(X.shape[0], 1)\n",
    "    return X\n",
    "    \n",
    "def log(num_array):\n",
    "    \"\"\" Logarithm extended to include 0 to avoid log of 0 rounding errors.\"\"\"\n",
    "    offset = 1e-20\n",
    "    return np.log(num_array + offset)\n",
    "\n",
    "def check_input_validity(X, Y, layers):\n",
    "    \"\"\" Raise error if invalid input is passed to network training method.\"\"\"\n",
    "    try:\n",
    "        observations, features = X.shape\n",
    "        label_size = Y.size\n",
    "    except AttributeError:\n",
    "        raise AttributeError(\"X and Y must be numpy arrays, \"\n",
    "                             \"or pandas data frames/series.\")\n",
    "    if not observations == label_size:\n",
    "        raise ValueError(\"Number of rows in X does not match \"\n",
    "                         \"number of labels.\")\n",
    "    if not features == layers[0]:\n",
    "        raise ValueError(\"Number of features in X does not match \"\n",
    "                         \"first entry of layers.\")\n",
    "    unique_labels = Y.unique()\n",
    "    if not set(unique_labels) <= set(range(layers[-1])):\n",
    "        raise ValueError(\"Labels in Y must be numbers from 0 and n, \"\n",
    "                         \"where n is the final entry of layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\Saigyouji\\Documents\\Python\\Data Science\\MNIST\\mnist_train.csv\", \n",
    "                    header=None)\n",
    "test = pd.read_csv(r\"C:\\Users\\Saigyouji\\Documents\\Python\\Data Science\\MNIST\\mnist_test.csv\", \n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column consists of the classification (a numeral from 0 to 9), so we split that into our Y array and throw the rest into X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has 60000 observations with 784 features.\n"
     ]
    }
   ],
   "source": [
    "train_X = train.iloc[:,1:]\n",
    "train_Y = train.iloc[:,0]\n",
    "test_Y = test.iloc[:,0]\n",
    "test_X = test.iloc[:,1:]\n",
    "print(\"The training data has {0} observations with {1} features.\"\n",
    "     .format(train_X.shape[0],train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our desired neural network (with 500 hidden layers) could take a few hours, so it will help to trim down the dimension a bit. A convenient feature of this dataset is that some preprocessing has been done, which includes centralizing the image. This means the outer pixels will have little or no information, and can be omitted without hurting the accuracy of our model. To see this, we can look at the variance of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      784.000000\n",
       "mean      4373.017134\n",
       "std       4977.789551\n",
       "min          0.000000\n",
       "25%         21.009916\n",
       "50%       1359.711250\n",
       "75%      10144.830434\n",
       "max      12953.412473\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variances = np.var(train_X, axis = 0)\n",
    "variances.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 25% of the pixels have a variance that is less than 0.5% of the mean variance. To confirm that these are indeed boundary pixels, we can perform the following quick hack to visualize where the low variance pixels are. (Note: on a sufficiently small screen, the rows of the matrix may wrap.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "low_variances = variances < 21.01\n",
    "print(np.reshape(low_variances * 1, (28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove the low variance pixels, and train our model. The layers vector determines the structure of the neural network. The first entry is the number of features, the final is the number of classes, and the entries inbetween give the size of each hidden layer. We'll use a fairly simple network: one hidden layer with 800 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.050263\n",
      "         Iterations: 200\n",
      "         Function evaluations: 261\n",
      "         Gradient evaluations: 261\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X.loc[:,np.invert(low_variances)]\n",
    "test_X = test_X.loc[:,np.invert(low_variances)]\n",
    "layers = [train_X.shape[1], 800, 10]\n",
    "network = train_network(train_X, train_Y, layers, regularization = 1)\n",
    "train_predictions = network.predict(train_X)\n",
    "test_predictions = network.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.9963833333333333\n",
      "Accuracy on test set: 0.9684\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = (train_Y == train_predictions).sum() / train_Y.shape[0]\n",
    "test_accuracy = (test_Y == test_predictions).sum() / test_Y.shape[0]\n",
    "print(\"Accuracy on train set: {0}\".format(train_accuracy))\n",
    "print(\"Accuracy on test set: {0}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy is high, but far from perfect. A bit of exploratory analysis may be of interest to see if the model suffers from specific deficiencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEKCAYAAADdBdT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAZZ3pLUIMtbkhpkeUtSgyxvSWrQ/wd6oMgn4txP3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8ab06d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_mistakes = test_Y.loc[test_Y != test_predictions]\n",
    "plt.xticks(range(10))\n",
    "plt.title(\"Misclassifications\")\n",
    "plt.hist(list(test_mistakes), bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we're classifying 0 and 1 relatively accurately, with our biggest issue being 9. We can construct similar plots to see what we're misclassifying some of these numbers as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEKCAYAAAAyx7/DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8ac9240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEKCAYAAAAyx7/DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFOlJREFUeJzt3X20JHV95/H3B4anEWYAiQ/o4CgRH7JslMNhiQa5IZiA\nj8dNDOIjarJr1lVMokeJHp3NJmvcHAOsxt3VABsQMAurHnGjQV2vwXWVEEHAgRhB5PlJYHjSBJjv\n/lE10Bn63rm3596p/o3v1zl9bndVddW3q7o//atfVd9KVSFJascOQxcgSVocg1uSGmNwS1JjDG5J\naozBLUmNMbglqTEG90+hJP81yfu24vnHJblgKWvabP5/leR1I4//MMltSW5MsibJPUmyDMu9J8na\npZ7vpJL8dpJbktydZK8x41+R5Lp+/HOSXJ7kBUPUqm0rnse9/UhyDfBEYN+q+tHI8IuBnwfWVtW1\nS7Cc44A3V9VhWzuvBSxrP+BKYM3oa1qC+c4CZ1TVKUs1z6WUZCdgA3BIVV0+xzRXAe+oqvO2aXEa\nnC3u7UsBVwPHbhqQ5EBgt35ci/YDfrSUod2b9vXxBGBX4IpxI/s9jv2A9duyKE0Hg3v780ng9SOP\n3wCcDjzctZDkfyT5j/39fZJ8PsmdSX6U5G82dUP03RKfTnJrktuTfGTcApOcnOTaJBuSXJTkF0fG\nHdIP25Dk5iQf7ofvmuST/XzvTHJhkp/px80meXOSXwbOB/btuzFOTbI2ycYkO/TT7p3ktCQ3JLkj\nyWf64Xv1r+vWfvh5SZ7Uj/sj4DDgo/18/0s/fGOSp/X3Vyc5vX/+NUneO7Jejkvy9SR/0s/76iRH\njbzm45Jc1XdhXJ3k1XOst12SnNTXfkOSE5PsnOQAHgnsu5J8efPnAfcAOwLfSfIP/fBrkhyRZN8k\n9492ryR5bt/dtGP/+E1J1vf1f7Hfs9k07Yl9F82GJJcm+blx9Ws4Bvf255vAqiTP7D+kx9CF+aji\nkRbn7wHXAfsAjwNOqKrqn/t54AfAU4AnAWfPscwL6bpi9gLOAs5JsnM/7mTgxKpaDTwN+Mt++BuA\nVcCTgb2Bfwv8ZLS+qvoKcDRwY1XtUVVvGrPsM+haps/u6//TfniAU+hapfsBPwY+Sjfj9wIXAG/t\n5/v2MfP9CLAH8FTgcLovwzeOjD+ErgvnscB/7pdFksf0r/moqloF/AJwyRzr7b39fH6+vx0CvK+q\nvgdsCsvVVXXk6JOq6h+ravf+4b+sqqdvGtWPvxH4f8CvjTzt1cA5VfVQkpcDJwCvoNvuF9Bv2yS/\nSvel9vR+m70SWOq9HW0lg3v7dAZd0LyQblf6hnmm/Se6fvG1VfVQVf3ffvgh/fB3VdWP+7D4xrgZ\nVNWZVXVnVW2sqj8FdgGeMTL/pyfZp6rur6oLR4Y/li4gqqourqp7xsx+zoOQSZ4IHAW8pao2VNWD\nVXVBX9MdVfWZqvpJVd0L/Ce6AN7ivEe+8E6oqvuq6ofAh4HXjUz2w6o6pbqDRKcDT0zyuH7cRuDA\nJLtV1S1VNVd3xquBP6iq26vqduA/jCxjaw++nkXfZdbvKRzTDwN4C/DBqvr7qtoIfBB4Tt/q/ie6\nL6xnJdmhn+bmraxFS8zg3v4UXXC/hjHdJCM2DfsT4PvA+f3u/bv74WvowmnjlhaY5J39bvddSe4E\nVtO15ADeDBwAXNF3h7y4H34G8NfAp/pugg8lWbHI17oGuKOqNoypaWWS/953H2wAvgas3tTd0Zur\nn3sfYCfghyPDrqXb69jk4TCrqvv7u7tX1X10IfkW4Ma+u+YZjLfvmGXsO8e0i/Vp4BeSPAF4AbCx\nqr7ej3sKcHLfRXUnj7So962qr9LtmfwZcEu/DvdYopq0RAzu7VB/5sjVdN0Mn97CtPdW1Turan/g\nZcDvJjmCLkT229QnOpckhwHvAl5ZVXtW1V50Z0Okn//3q+rVVfUzwIeAc/uW6INV9QdV9XPA84CX\n8M/75hfiOmDvJKvHjPs9ui+MQ/pd/sP7mjYF93wHJ28HHgDWjgzbD7h+IUVV1flV9St0BxivBD4x\nx6Q3jlnGjQtZxgJquJPu+MAxdC370W6ua4F/U1V7jdweU1Xf7J/7kao6mK776QC67aspYnBvv94M\nHFFVPx4zbvRA5UuS/GzfEr0beKi/XQjcBPxx33rdNcnzxsxrD+BB4Pb+wNr76fquN83/tZsOOtIF\negEbk/xSkgP7L4Z76ILyocW8wKq6CfgC8LEkeybZqf8iAdidrl97Q5K9gQ9s9vRbgP3nmO9DwP8E\n/ijJ7kmeAvwOjz5W8ChJHpfk5X1f9wPAffO8rrOB96U7QLwP8H66PZGlchbdXtev8Ug3CcB/A34/\nybP7mlcneWV//+Ak/yrd6Yj30x13WNR20fIzuLdTVXV1VX17dNBm9zc9/lngS3Th+Q3gz6rqa30X\nyUv78dfStW5/Y8zzv9jfvgdcQxeWo+eK/ypweZJ7gBOBV1XVPwKPB86hC/P1wCxzh9bmrePRx6+j\nC8gr6cL4+H74SXSnQd7ev64vbPa8k4Ff78+qOGnMMt9GF7pX0x28OxM4bczr37ymHehC/ga6LojD\ngN+e43X9IXARcGl/u6gfNu51jrOl8Z+j2343VdVlDz+p6rN0ez+f6ruRLqPbTtB96X4cuINue95O\n152mKTLvD3CSnAq8GLi1qg7sh+1Nd2bAU+g27G9U1V3LX6okCbbc4j6N7qj9qPcAX6qqA4Cv9I8l\nSdvIFn/ynu5/N5w30uK+Eji8qm7pj1jPVtUzl7tQSVJnkj7ux1fVLf39W+j6KiVJ28hWHZzsf3ww\n7f/zQZK2K4v9wQN0J+U/oapu7n+5duu4iZIY6JI0gaqa95ezk7S4P0d3bij938/Os/Cpun3gAx8Y\nvIYha+q3ygJvH1jEtIu5Tf6+2N623+K2x2Jui9l22+ZzOo3bblrrWoh5gzvJ2XTnwD4j3T9sfyPw\nx8ALk3wPOKJ/LEnaRubtKqmqY+cYdeQcwyVJy+yn6peTMzMzQ5fwKNNYU2dm6AIeZRrX1TTW5LZb\nuGmta0uW7dJlSWq55q3JdP+OZOhtkgX3423v3B4aJwm1DAcnJUkDMrglqTEGtyQ1xuCWpMYY3JLU\nGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0x\nuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINb\nkhpjcEtSYwxuSWqMwS1JjTG4JakxEwd3khOSfDfJZUnOSrLLUhYmSRpvouBOshb4LeCgqjoQ2BF4\n1dKVJUmay4oJn3c38ACwMslDwErghiWrSpI0p4la3FV1B/Bh4FrgRuCuqvryUhYmSRpvohZ3kv2B\ndwBrgQ3AOUleU1Vnjk63bt26h+/PzMwwMzMzaZ2StF2anZ1ldnZ2Uc9JVS16QUmOAV5YVb/ZP34d\ncGhVvXVkmppk3lo+SYCht0nwfdFxe2icJFRV5ptm0rNKrgQOTbJbunffkcD6CeclSVqESfu4vwOc\nDlwEXNoP/vhSFSVJmttEXSULmrFdJVPHXfPp4vbQOMvZVSJJGojBLUmNMbglqTEGtyQ1xuCWpMYY\n3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEt\nSY1ZMXQBktRdxm14rVzGzeCWNCWGDs3p+PJYCLtKJKkxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMM\nbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaszEwZ1kzyTnJrkiyfokhy5l\nYZKk8bbmQgonA39VVb+eZAXwmCWqSZI0j0xyqZ4kq4GLq+pp80xTrVwG6KdFd3moobdJmrk81HJz\ne4xU4bp4pIqEqpr3cjyTdpU8FbgtyWlJvp3kE0lWTjgvSdIiTBrcK4CDgI9V1UHAfcB7lqwqSdKc\nJu3jvh64vqr+tn98LmOCe926dQ/fn5mZYWZmZsLFSdL2aXZ2ltnZ2UU9Z6I+boAkfwP8ZlV9L8k6\nYLeqevfIePu4p4z9iNPF7TFShevikSoW0Me9NWeVvA04M8nOwFXAG7diXpKkBZq4xb3FGdvinjq2\naqaL22OkCtfFI1Us41klkqSBGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4Jakxhjc\nktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1J\njTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQY\ng1uSGmNwS1Jjtiq4k+yY5OIk5y1VQZKk+W1ti/t4YD1QS1CLJGkBJg7uJE8GXgT8OZAlq0iSNK+t\naXGfCLwL2LhEtUiSFmCi4E7yEuDWqroYW9uStE2tmPB5zwNeluRFwK7AqiSnV9XrRydat27dw/dn\nZmaYmZmZcHGStH2anZ1ldnZ2Uc9J1dYdV0xyOPDOqnrpZsNra+etpZWE4Y8jB98XHbfHSBWui0eq\nSKiqeXsyluo87uFfrST9lNjqFvecM7bFPXVs1UwXt8dIFa6LR6rYhi1uSdI2YnBLUmMMbklqjMEt\nSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLU\nGINbkhoz6cWCF+Too49Zztlv0c47w9lnn8bKlSsHrUOSltKyXroMPrUs816oHXc8jjvuuIVVq1YN\nWse08PJQ08XtMVLFlKyLabGlS5ctc3APuyF23nkVt912vcHdm5YPxzQExTRwe4xUMSXrYvgaoN8m\nXnNSkrYnBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQY\ng1uSGmNwS1JjDG5JaozBLUmNmSi4k6xJ8tUk301yeZK3L3VhkqTxJr3m5APA71TVJUl2B/4uyZeq\n6oolrE2SNMZELe6qurmqLunv3wtcAey7lIVJksbb6j7uJGuB5wLf2tp5SZK2bNKuEgD6bpJzgeP7\nlvdm1o3cn+lvkqZJd6FeDWe2vy3cxFd5T7IT8HngC1V10pjxXuV9ykzLlbSn4ari02BatsfwNcB0\n1DENNcCyXeU93TvuFGD9uNCWJC2fSfu4nw+8FvilJBf3t6OWsC5J0hwm6uOuqq/jj3ckaRCGryQ1\nxuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMM\nbklqjMEtSY0xuCWpMQa3JDVmq67y3oLVq1cPXYI2Mw1XFfeCxWrZdh/cnaE/pNNz9ejpMPS6mJb1\nIE3GrhJJaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrck\nNcbglqTGGNyS1BiDW5IaY3BLUmMmDu4kRyW5Msk/JHn3UhYlSZrbRMGdZEfgo8BRwLOBY5M8aykL\nWx6zQxcwxuzQBcxhdugCxpgduoBHmZ2dHbqEMWaHLmCM2aELmMPs0AVMZNIW9yHA96vqmqp6APgU\n8PKlK2u5zA5dwBizQxcwh9mhCxhjdugCHsXgXqjZoQuYw+zQBUxk0uB+EnDdyOPr+2GSpGU26cWC\nF3S111WrXjrh7JfGvffeP+jyJWk5pGrxV9xOciiwrqqO6h+fAGysqg+NTDP0pbwlqUlVlfnGTxrc\nK4C/B34ZuBG4EDi2qq6YpEhJ0sJN1FVSVQ8m+ffAXwM7AqcY2pK0bUzU4pYkDWdZfjk5bT/OSXJq\nkluSXDZ0LZskWZPkq0m+m+TyJG+fgpp2TfKtJJckWZ/kg0PXtEmSHZNcnOS8oWvZJMk1SS7t67pw\n6HoAkuyZ5NwkV/Tb8NCB63lGv3423TZMyXv9hP6zd1mSs5LsMgU1Hd/Xc3mS4+eduKqW9EbXdfJ9\nYC2wE3AJ8KylXs4iazoMeC5w2ZB1bFbTE4Dn9Pd3pztmMOh66mtZ2f9dAXwT+MWha+rr+V3gTOBz\nQ9cyUtMPgL2HrmOzmv4CeNPINlw9dE0jte0A3ASsGbiOtcDVwC79478E3jBwTf8CuAzYtc/QLwH7\nzzX9crS4p+7HOVV1AXDnkDVsrqpurqpL+vv3AlcA+w5bFVTVpnMod6Z7A90xYDkAJHky8CLgz4F5\nj7YPYGrqSbIaOKyqToXuWFRVbRi4rFFHAldV1XVbnHJ53Q08AKzsT7RYCdwwbEk8E/hWVf2kqh4C\nvgb867kmXo7g9sc5i5RkLd0ewbeGrQSS7JDkEuAW4KtVtX7omoATgXcBG4cuZDMFfDnJRUl+a+hi\ngKcCtyU5Lcm3k3wiycqhixrxKuCsoYuoqjuADwPX0p0Vd1dVfXnYqrgcOCzJ3v02ezHw5LkmXo7g\n9mjnIiTZHTgXOL5veQ+qqjZW1XPo3jQvSDIzZD1JXgLcWlUXM0Wt297zq+q5wNHAW5McNnA9K4CD\ngI9V1UHAfcB7hi2pk2Rn4KXAOVNQy/7AO+i6TPYFdk/ymiFrqqorgQ8B5wNfAC5mnobKcgT3DcCa\nkcdr6Frd2kySnYD/BXyyqj47dD2j+l3s/w0cPHApzwNeluQHwNnAEUlOH7gmAKrqpv7vbcBn6LoJ\nh3Q9cH1V/W3/+Fy6IJ8GRwN/16+roR0MfKOqflRVDwKfpnufDaqqTq2qg6vqcOAuuuNeYy1HcF8E\nPD3J2v5b9hjgc8uwnKYlCXAKsL6qThq6HoAk+yTZs7+/G/BCum/+wVTV71fVmqp6Kt2u9v+pqtcP\nWRNAkpVJ9ujvPwb4FbqDS4OpqpuB65Ic0A86EvjugCWNOpbui3caXAkcmmS3/nN4JDB4l2CSx/V/\n9wNewTzdSpP+r5I51RT+OCfJ2cDhwGOTXAe8v6pOG7Im4PnAa4FLk2wKxxOq6osD1vRE4C+S7ED3\npX5GVX1lwHrGmZauuMcDn+k+96wAzqyq84ctCYC3AWf2jaargDcOXM+mL7YjgWk4DkBVfaffa7uI\nrjvi28DHh60KgHOTPJbuwOm/q6q755rQH+BIUmO8dJkkNcbglqTGGNyS1BiDW5IaY3BLUmMMbklq\njMEtSY0xuCWpMf8f64guvuxam8QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8b57400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = Series(test_predictions)\n",
    "\n",
    "test_mistakes_nines = test_predictions.loc[(test_Y != test_predictions) & (test_Y == 9)]\n",
    "plt.figure(0)\n",
    "plt.xticks(range(10))\n",
    "plt.title(\"Misclassifications of nines\")\n",
    "plt.hist(list(test_mistakes_nines), bins = 10);\n",
    "\n",
    "test_mistakes_fives = test_predictions.loc[(test_Y != test_predictions) & (test_Y == 5)]\n",
    "plt.figure(1)\n",
    "plt.xticks(range(10))\n",
    "plt.title(\"Misclassifications of fives\")\n",
    "plt.hist(list(test_mistakes_fives), bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, we see mistakes concentrated between numbers that look similar, such as 5 and 3, and 9 and 4. Beyond that, these graphs suggest that the model does not suffer specific defects. Instead, the model is simply imperfect at distinguishing between edge cases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
