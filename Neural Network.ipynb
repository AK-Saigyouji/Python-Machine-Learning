{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick demonstration of the neural network implementation given in this repository. We use the well known MNIST handwritten numeral dataset, which consists of 28 pixel by 28 pixel images of handwritten numerals from 0 to 9. The data thus has 784 features, each being an intensity value from 0 (white) to 255 (black). There are 70000 images in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is included here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel():\n",
    "    \"\"\" Output of a neural network training algorithm. \"\"\"\n",
    "    def __init__(self, neural_network):\n",
    "        self._network = neural_network\n",
    "        \n",
    "    def get_weights(self):\n",
    "        \"\"\" Return the weights used by the model to make predictions.\"\"\"\n",
    "        return self._network.weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify the observations in X.\n",
    "        \n",
    "        Args: \n",
    "            X (array): data to be classified. \n",
    "        Returns:\n",
    "            array: each entry is the class assigned to the corresponding\n",
    "            row in X.\n",
    "        Raises:\n",
    "            ValueError: if the features in X do not match the model. \n",
    "        \"\"\"\n",
    "        \n",
    "        num_features = self._network.layers[0]\n",
    "        if not X.shape[1] == num_features:\n",
    "            raise ValueError(\"Number of features in data must be {0}.\"\n",
    "                             .format(num_features))\n",
    "        forward_propogate(self.get_weights(), X, self._network)\n",
    "        class_predictions = self._network.get_final_activation()\n",
    "        return np.argmax(class_predictions, axis = 1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(\"Trained neural network with {0} hidden layers of size(s) {1}.\"\n",
    "               .format(len(self._network.layers)-2, self._network.layers[1:-1]))\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\" Internal network class to keep track of its properties.\"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = layer_sizes\n",
    "        self._shapes = []\n",
    "        self.activations = []\n",
    "        self.raw_outputs = []\n",
    "        self._initialize_shapes()\n",
    "        self.weights = None\n",
    "        \n",
    "    def num_transitions(self):\n",
    "        \"\"\" Return the number of transitions between layers.\"\"\"\n",
    "        return len(self.layers) - 1\n",
    "    \n",
    "    def get_final_activation(self):\n",
    "        \"\"\" Return the last activation, giving the current probabilities.\"\"\"\n",
    "        return self.activations[self.num_transitions()]\n",
    "        \n",
    "    def _initialize_shapes(self):\n",
    "        \"\"\" Determine the shapes for the weights in the network.\"\"\"\n",
    "        for index in range(self.num_transitions()):\n",
    "            input_size = self.layers[index]\n",
    "            output_size = self.layers[index+1]\n",
    "            shape = (output_size, 1 + input_size)\n",
    "            self._shapes.append(shape)\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\" Return random initial weights according to this network's shapes.\"\"\"\n",
    "        weights = []\n",
    "        for index in range(self.num_transitions()):\n",
    "            init_epsilon = self._get_initial_epsilon(index)\n",
    "            shape_x, shape_y = self._shapes[index]\n",
    "            weight = init_epsilon * (np.random.rand(shape_x, shape_y) * 2 - 1)\n",
    "            weights.append(weight)\n",
    "        return weights\n",
    "        \n",
    "    def _get_initial_epsilon(self, index):\n",
    "        \"\"\" Return scaling factor for initialized weights.\"\"\"\n",
    "        input_size = self.layers[index]\n",
    "        output_size = self.layers[index+1]\n",
    "        return (6 / (input_size + output_size))**(1/2)\n",
    "        \n",
    "    def reshape_weights(self, flat_weights):\n",
    "        \"\"\" Restore shape of weights according to this network's shapes.\"\"\"\n",
    "        start_index = 0\n",
    "        shaped_weights = []\n",
    "        for shape in self._shapes:\n",
    "            matrix_size = shape[0] * shape[1]\n",
    "            end_index = start_index + matrix_size\n",
    "            shaped_weights.append(flat_weights[start_index:end_index].reshape(shape))\n",
    "            start_index = end_index\n",
    "        return shaped_weights\n",
    "        \n",
    "def train_network(X, Y, layers, regularization = 0, max_iters = 200):\n",
    "    \"\"\" Train a neural network and return the model.\n",
    "    \n",
    "    Args:\n",
    "        X (array): data consisting of rows of features. \n",
    "        Y (array): array of labels corresponding to each row in X.\n",
    "            Must consist of integers from 0 to n for some integer n.\n",
    "        layers (list): the number of features in each layer. The first\n",
    "            entry must be the number of features (columns) in X, the \n",
    "            last must be the number of classes, and those inbetween\n",
    "            determine the size of each hidden layer.\n",
    "        regularization (int): penalty factor for having larger weights.\n",
    "            (defualt: 0).\n",
    "        max_iters (int): the max number of iterations used by the algorithm\n",
    "            when searching for optimal weights. A higher number will produce\n",
    "            a better fit but extends run time (default: 200).\n",
    "    \"\"\"\n",
    "    check_input_validity(X, Y, layers)\n",
    "    num_classes = layers[-1]\n",
    "    network = NeuralNetwork(layers)\n",
    "    initial_weights = flatten_weights(network.initialize_weights()) \n",
    "    Y = process_labels(Y, num_classes)\n",
    "    optimal = fmin_cg(compute_cost, \n",
    "                      initial_weights, \n",
    "                      back_propogate, \n",
    "                      args = (X, Y, network, regularization), \n",
    "                      maxiter = max_iters)\n",
    "    forward_propogate(network.reshape_weights(optimal), X, network)\n",
    "    network.weights = network.reshape_weights(optimal)\n",
    "    return NeuralNetModel(network)\n",
    "    \n",
    "def compute_cost(flat_weights, X, label_matrix, network, regularization):\n",
    "    \"\"\" Propogate weights through network and compute cost function.\"\"\"\n",
    "    weights = network.reshape_weights(flat_weights)\n",
    "    forward_propogate(weights, X, network)\n",
    "    return cost_function(weights, label_matrix, network, regularization)\n",
    "    \n",
    "def forward_propogate(weights, X, network):\n",
    "    \"\"\" Perform forward propogation on the given network and dataset.\"\"\"\n",
    "    raw_outputs = [X]\n",
    "    activations = [X]\n",
    "    for i in range(network.num_transitions()):\n",
    "        activations[i] = insert_ones(activations[i])\n",
    "        weight = weights[i]\n",
    "        raw_output = activations[i].dot(weight.transpose())\n",
    "        activation = sigmoid(raw_output)\n",
    "        raw_outputs.append(raw_output)\n",
    "        activations.append(activation)\n",
    "    network.activations = activations\n",
    "    network.raw_outputs = raw_outputs\n",
    "        \n",
    "def cost_function(weights, label_matrix, network, regularization):\n",
    "    \"\"\" Compute the cost function for the network's current state.\"\"\"\n",
    "    a = network.get_final_activation()\n",
    "    Y = label_matrix\n",
    "    m = len(label_matrix)\n",
    "    weight_sum = 0\n",
    "    for weight in weights:\n",
    "        weight_sum += (weight[:,1:]**2).sum()\n",
    "    reg_term = (regularization / (2*m)) * weight_sum\n",
    "    return (-Y * log(a) - (1-Y) * log(1-a)).sum() / m + reg_term\n",
    "    \n",
    "def back_propogate(flat_weights, X, label_matrix, network, regularization):\n",
    "    \"\"\" Use back propogation to get the gradient of the cost function.\"\"\"\n",
    "    weights = network.reshape_weights(flat_weights)\n",
    "    #Todo: clean up algorithm so this step isn't necessary\n",
    "    if len(network.activations) == 0:\n",
    "        forward_propogate(weights, X, network)\n",
    "    deltas = get_deltas(weights, label_matrix, network)\n",
    "    weight_gradients = get_weight_gradients(weights, deltas, network, regularization)\n",
    "    return weight_gradients\n",
    "\n",
    "def get_deltas(weights, label_matrix, network):\n",
    "    \"\"\" Return a list of the deltas needed for the gradient computation.\"\"\"\n",
    "    deltas = []\n",
    "    delta = network.get_final_activation() - label_matrix\n",
    "    deltas.append(delta)\n",
    "    for index in reversed(range(1, network.num_transitions())):\n",
    "        weight = weights[index][:,1:]\n",
    "        sigmoid_grad = sigmoid_gradient(network.raw_outputs[index])\n",
    "        delta = delta.dot(weight) * sigmoid_grad\n",
    "        deltas.insert(0, delta)\n",
    "    return deltas\n",
    "\n",
    "def get_weight_gradients(weights, deltas, network, regularization):\n",
    "    \"\"\" Return a flat array of the gradients of the weights.\"\"\"\n",
    "    activations = network.activations\n",
    "    weight_gradients = []\n",
    "    m = activations[0].shape[0]\n",
    "    for index, weight in enumerate(weights):\n",
    "        weight[:,0] = 0\n",
    "        delta, activation = deltas[index], activations[index]\n",
    "        base_term = delta.transpose().dot(activation) / m\n",
    "        reg_term = regularization * weight / m\n",
    "        weight_gradients.append(base_term + reg_term)\n",
    "    return flatten_weights(np.array(weight_gradients))\n",
    "\n",
    "def flatten_weights(weights):\n",
    "    \"\"\" Return a flat array of the weights.\"\"\"\n",
    "    return np.concatenate([weight.flatten() for weight in weights])\n",
    "\n",
    "def process_labels(Y, num_labels):\n",
    "    \"\"\" Given a sequence of labels 0 to n, produce a 0-1 matrix where entry\n",
    "    i, j is 1 if and only if the ith label is j.\"\"\"\n",
    "    label_matrix = np.zeros((len(Y), num_labels))\n",
    "    for i in range(num_labels):\n",
    "        label_matrix[:,i] = 1 * (Y == i)\n",
    "    return label_matrix\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    \"\"\" Gradient of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\" Vectorized sigmoid/logistic function.\"\"\"\n",
    "    return expit(z)\n",
    "    \n",
    "def insert_ones(X):\n",
    "    \"\"\" Insert a column of ones in front of the dataset X and return it.\"\"\"\n",
    "    X = array_to_ndarray(X)\n",
    "    num_rows = X.shape[0]\n",
    "    return np.hstack((np.ones((num_rows, 1)), X))\n",
    "    \n",
    "def array_to_ndarray(X):\n",
    "    \"\"\" Return a multidimensional version of X if it isn't already one.\"\"\"\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(X.shape[0], 1)\n",
    "    return X\n",
    "    \n",
    "def log(num_array):\n",
    "    \"\"\" Logarithm extended to include 0 to avoid log of 0 rounding errors.\"\"\"\n",
    "    offset = 1e-20\n",
    "    return np.log(num_array + offset)\n",
    "\n",
    "def check_input_validity(X, Y, layers):\n",
    "    \"\"\" Raise error if invalid input is passed to network training method.\"\"\"\n",
    "    try:\n",
    "        observations, features = X.shape\n",
    "        label_size = Y.size\n",
    "    except AttributeError:\n",
    "        raise AttributeError(\"X and Y must be numpy arrays, \"\n",
    "                             \"or pandas data frames/series.\")\n",
    "    if not observations == label_size:\n",
    "        raise ValueError(\"Number of rows in X does not match \"\n",
    "                         \"number of labels.\")\n",
    "    if not features == layers[0]:\n",
    "        raise ValueError(\"Number of features in X does not match \"\n",
    "                         \"first entry of layers.\")\n",
    "    unique_labels = Y.unique()\n",
    "    if not set(unique_labels) <= set(range(layers[-1])):\n",
    "        raise ValueError(\"Labels in Y must be numbers from 0 and n, \"\n",
    "                         \"where n is the final entry of layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\Saigyouji\\Documents\\Python\\Data Science\\MNIST\\mnist_train.csv\", \n",
    "                    header=None)\n",
    "test = pd.read_csv(r\"C:\\Users\\Saigyouji\\Documents\\Python\\Data Science\\MNIST\\mnist_test.csv\", \n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column consists of the classification (a numeral from 0 to 9), so we split that into our Y array and throw the rest into X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = train.iloc[:,1:]\n",
    "train_Y = train.iloc[:,0]\n",
    "test_Y = test.iloc[:,0]\n",
    "test_X = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers vector determines the structure of the neural network. The first entry is the number of features, the final is the number of classes, and the entries inbetween give the size of each hidden layer. We'll use a fairly simple network: one hidden layer with 500 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.239013\n",
      "         Iterations: 200\n",
      "         Function evaluations: 243\n",
      "         Gradient evaluations: 243\n"
     ]
    }
   ],
   "source": [
    "layers = [784, 500, 10]\n",
    "network = train_network(train_X, train_Y, layers, regularization = 1)\n",
    "train_predictions = network.predict(train_X)\n",
    "test_predictions = network.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the accuracies that we have some overfitting so if we were trying to optimize the model we could try increasing the regularization parameter to improve the test set accuracy further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.96905\n",
      "Accuracy on test set: 0.9483\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = (train_Y == train_predictions).sum() / 60000\n",
    "test_accuracy = (test_Y == test_predictions).sum() / 10000\n",
    "print(\"Accuracy on train set: {0}\".format(train_accuracy))\n",
    "print(\"Accuracy on test set: {0}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model above took a significant amount of time, which limits our ability to try more complex networks on this data set. We could significantly speed things up by reducing the dimension of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 67 columns where every observation is 0.\n",
      "There are 274 columns where the average intensity is less than 5.\n"
     ]
    }
   ],
   "source": [
    "trivial_cols = (train_X.max() == 0).sum()\n",
    "low_var_cols = ((train_X.sum(axis = 0))/10000 < 5).sum()\n",
    "print(\"There are {0} columns where every observation is 0.\".format(trivial_cols))\n",
    "print(\"There are {0} columns where the average intensity is less than 5.\".format(low_var_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see, a significant proportion of pixels are always white, and a larger proportion have very low variance. The former provide no information and are thus useless, while the latter could be removed with minimal loss of information. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
